---
layout: post
title: A cookbook for maintainable physics code
subtitle: What I wish I knew 3 years ago
tags: [code, comp-sci]
katex: true
---

Generations of physicists have perpetuated the idea that simulation writing and software engineering
are somehow orthogonal skills, and that the latter is somehow not needed in the realm of real code.

I disagree.

There are three essential reasons that scientists _should care about following good practices_:

1. Good science should be _independently verifiable_, so there is a moral obligation to make
   code accessible for others to read, understand and run to check your work.
2. Good code is _easier to modify_- investing a few weeks into setting the problem up robustly can
   translate to massive time savings later (at least in principle)
3. Good practices make you _more confident that the result you have is correct_: Making code modular
   allows you to test different subsections independently, rooting out bugs before they cause weird
   behaviour somewhere else.


To see what I mean, I'll reference two toy models I want to simulate physically. In particular, I'll
define _Model Inputs_, which are the abstract ideas that the routine needs to grapple with, and
_Model Outputs_, the abstract notions that the code should somehow output. The details of how these
details are represented still need to be nailed down.

_Model A: A deterministic SchrÃ¶dinger equation solver._ 'Exactly' diagonalises the Hamiltonian for `n`
interacting particle in an arbitrary 1D potential well V(x).

Inputs: 
 + `V(x)`: The potential well.
 + `n`: The number of particles.
 + `n_states`: The number of eigenvalues we want to find (0 = ground state only)

Outputs:
 + Energy/ies
 + Wavefunction/s

_Model B: Spin-Wave Theory._  Find the classical ground state of a lattice, expand the ground state
in Holstein-Primakoff bosons, truncate to next-leading order and diagonalise the Hamiltonian. Then
predict the spin correlators seen by neutron scattering.

Inputs:
 + `Lattice`, describing the sites and connectivity of the magnetic sites
 + `corrtype`, specifying which correlators to probe (the relevant one depaands on the kind of spins
   being investigated)
 + `BZpath`, the path through the Brillouin zone to plot
 + `B`, the external, applied magnetic field

Outputs:
 + Magnon dispersion along a specific curve in $k$ space
 + Spectral weight as seen in a single crystal scattering experiment
 + Spectral weight as seen by a powder diffraction experiment

## The Zen of Physics Code

> Before I met Jeanine, my life was cosmically a shambles, it was ah... I was using bits and pieces of whatever
> Eastern philosophies happened to drift through my transom and she sort of sorted it out for me, straightened
> it out for me, gave me a path, you know, a path to follow.

 \- David St. Hubbins, _This is Spinal Tap_

 1. Make it obvious where data comes from.
 2. Make it obvious what programs do.
 3. Programs should do one thing and do it well.
 4. At most three shell commands should be involved in transforming parameters into a publishable
    graph.
 5. Label everything.

The general idea here is to write executables and datafiles that can be read as verbs and nouns
that describe the whole calculation:

```
[ Find the two-body Hamiltonian ] of ( two electrons ) and [ diagonalise it ]

[ Find the classical ground state ] of ( spins on a lattice ), [ expand to second order in
Holstein-Primakoff bosons ] and [ find the corresponding spin correlators ], then [ average over
them with Monte Carlo ]
```

Each item in `[ brackets ]` can be interpreted as a verb, and arguably deserves its own executable, and the item in `( parentheses )`
should have an input file associated with it. Obviously there's a fine art here to deciding how to
modularise everything - if the code is fast, it can be more appropriate to have a 'god object' that
does everything. These guidelines are intended for a fairly specific use case, a **complicated,
multi-step calculation** that technically only needs to run once. 

Less abstractly, these maxims translate to
 1. Every datafile in your output should have the input that made it and the tools to interpret it saved with it.
 2. Write your 'verbs' in such a way that they can be used without reading the source code.
    Instead of crashing when you use an incorrect call sequence, gently remind the user how it's
    meant to be called with a message. Use at most 4 command line parameters, any
    more and they should be in a file.
 3. Your code does not have to fit into one executable. If your simulation involves precalculating
    some parameters then running a MC simulation, you should have two executables `calc_params` and
    `run_sim`, where the latter is fed output from the former. The intermediate data should be saved to a file (with metadata), as per [1].
 4. Your project should have at most three levers - "run simulation", "process data" and "plot data". (A good way to
    achieve this is with shell or python scripts).
 5. Magic numbers - in the sorce code, _or in the inout files_, are not allowed.

# Directory structure

This is more art than science, but if you're writing your first big project it can seem an
insurmountable obstacle to even establish where to start. 

The thing to have in mind is this: For science code, the main tasks you'll be doing are
 1. Writing the simulation.
 2. Creating new simulations with slightly different parameters
 3. Plotting the output data in new, interesting ways

The goal now is to minimise the hassle that these tasks cause.

If you're going "unix style" where the data structure is represented with _system directories_, Model A might end up looking like

```
Model A

mdA + .gitignore
    + Makefile
    + README.md
    + _driver ----- + simulate.sh 
    + _process ---- + plot_wavefunc.py
    |               + plot_momentum.py
    |               + correlate.ipynb
    |               + img --------- + wavefun_onepart.pdf
    |                               + wavefun_twopart.pdf
    + bin --------- + simwavfunc
    |               + simspinfunc
    |               + spinwavfunc_field
    + src --------- + sim.h
    |               + sim.c
    |               + ...
    + input -----   + one_part.inp
    |               + two_part.inp
    |               + potentials -- + harmonic.csv
    |                               + quartic.csv
    |                               + free.csv
    + data   ------ + one_part ---- + _sim_params.inp
                    |               + _potential.csv
                    |               + rcorr.csv
                    |               + pcorr.csv
                    |               + wavefunc.csv
                    |               + energy.csv
                    + two_part ---- + _sim_params.inp
                                    + _potential.csv
                                    + rcorr.csv    
                                    + pcorr.csv
                                    + wavefunc.csv
                                    + energy.csv
```

I'm using underscores to push the most important things to the top of `ls` output.
Creating and running a simulation might then look like

```bash
cd mdA
cp input/two_part.inp input/three_part.inp
vi input/three_part.inp # edit the input parameters...
bin/simwavfunc input/one_part.inp data/one_part
# wait until finished...
python3 _process/plot_wavefunc.py data/one_part
```

The bash scripts `simulate.sh` and `plot.sh` are the last things to be
added before publishing. They contain, respectively
```bash
#!/bin/bash
# simulate.sh
mkdir data

for sim in `ls input`; do
    # remove the extension
    s= "${sim%.*}"
    mkdir "data/$s"
    bin/simwavfunc "input/$sim" "data/$s"
done
```

```bash
#!/bin/bash
# process.sh
mkdir _process/img
for dir in `ls data`; do
    python3 _process/plot_wavefunc.py "data/$dir"
done
```
These are the two levers of maxim [4].

Note `_sim_params.inp` in the output folders. These are copies of `one_part.inp` and `two_part.inp`, which
are _produced by the simulation executable_. This is helpful for many reasons:
- In case you change any of the input data while your code is running (very easy to do if it spans
   multiple days)
- To keep metadata with data (maxim [1])
- To reassure you that the program is reading the parameters correctly.
- To keep all the data that a post-processing script needs in one place.

Some extra features of this structure are:
- The programs involved never have to traverse system directories. This is helpful for code
   readability and cross-platform compatibility.
- The data folder could be called anything and located anywhere (even on a networked drive!) 




# File formats for input

It's really tempting to write input files quick and dirty. For example:
```
# input.in
# first run, single particle test in a harmonic trap
1 # number of particles
11 # number of sites
25 16 9 4 1 0 1 4 9 16 25 # potential (must have n = number of sites)
1e-4 # interaction strength
```

or even more mystically

```
1
11
25 16 9 4 1 0 1 4 9 16 25
1e-4
```

The last structure is particularly simple to parse in a C-like language -

```c++

#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define MAX_LINE_LEN 1024

void parse_file(
    const char* fname,
    uint16_t* nparts,
    uint16_t* nsites,
    double** V,
    double* istrength
    ) {

    FILE* fp = fopen(fname, "r");
    char linebuf[MAX_LINE_LEN];
    // read first line
    fgets(linebuf, MAX_LINE_LEN, fp);
    *nparts = atoi(linebuf);
    // read second line
    fgets(linebuf, MAX_LINE_LEN, fp);
    *nsites = atoi(linebuf);

    *V = (double *) malloc(sizeof(double)* (*nsites));
    // read third line V into memory
    fgets(linebuf, MAX_LINE_LEN, fp);
    // assume space delimiters
    char *token = strtok(linebuf, " ");

    uint16_t idx=0;
    while( token != NULL ) {
        (*V)[idx] = atof(token);
        token = strtok(NULL, " ");
        idx++;
        if (idx >= *nsites) break; // make sure we don't overrun!
    }

    // read fourth line
    fgets(linebuf, MAX_LINE_LEN, fp);
    *istrength = atof(linebuf);
}

```

From a scientific perspective, the code is very dangerous. Suppose you miscount and write 

```
1
15
25 16 9 4 1 0 1 4 9 16 25
1e-4
```

Then the program will *silently* allocate more space than is needed for `V`, and will set them to
zero (if the OS is being polite) or to whatever crap was lying in the ram before your program got hold of
it!

Even worse is if we forget the line order:

```
1
1e-4
25 16 9 4 1 0 1 4 9 16 25
11
```

It's not clear what `atoi` will do now, but it definitely isn't the right thing.

None of these issues are unfixable, it's just a massive timesink trying to sort these annoying
administrative issues out. I propose a few ideas for alternatives below.

## Rolling your own parser code

This can be defensible if you only have a small number of low-complexity inputs. There's something
to be said for avoiding dependencies on libraries. Some guidelines for doing this though:

 - Write the input file _before_ you write the parser.
 - Use natural-feeling delimiters like `=`, `:`
 - Do it once, do it write, never worry about it again.

My own homebrew shitty parser is on GitHub if you;re interested. Its interface works like this:

```c++
#include <parameter.hh>

int main (int argc, char** argv) {
    // ---------- Reading parameters ----------
    // Required parameters
    if (argc < 4)
        throw "Required parameters missing";

    int n;
    double T;
    
    // Optional parameter variables
    unsigned
        sample = 2048,
        sweep  = 4,
        burnin = 128,        
        n_time = 512, 
        n_freq = 128; 
    double
        delta = 0.0625;
    

    param_t p;
    p.declare("system_size",n);
    p.declare("temperature",T);
    p.declare("n_sample", sample);
    p.declare("sweep", sweep);
    p.declare("n_burnin", burnin);
    p.declare("n_timesteps", n_time);
    p.declare("n_Egrid", n_freq);
    p.declare("dt", delta);

    double gphase=0;
    p.declare("theta", gphase);

    //read the params
	p.from_file(argv[1]);


	// ... simulation code uses sample, sweep, burnin....


	std::string param_copy_path(argv[2]);
	param_copy_path.append('/params.inp');
	p.save_file(param_copy_path);
	return 0;
}

**Pros**
- Very flexible
- Can be quicker to get running if you know what you're doing
- Can implement data validation that errors if invalid data is fed in

**Cons**
- Lots of ways to mess it up
- Tradeoff between readability of the code and readability of the input file


```
## JSON / YAML / TOML

Babies of Web 2.0, these standards are widely supported and expert at handling heterogeneous data.
The syntax for YAML and TOML is a bit cleaner than JSON, but the three are otherwise essentially
interchangeable. For a Rosetta stone of sorts, see [this deep
dive.](https://gohugohq.com/howto/toml-json-yaml-comparison/)

**Pros**
- Clean syntax
- Handles heterogeneous data very naturally

**Cons**
- Generally requires using a library (e.g. `nlohmann/json` in C++), which can get annoying in
compiled languages
- No data validation by default, this also has to be done by hand

## 




# File formats for output

## CSV

**Pros:** Portable, simple to implement, easily readable by anyone, metadata can live in the file

**Cons:** Slow IO, limited to 2D data, documentation must be done with a "hack" in a comment

It is generally desirable to use common, human-readable data formats.
`.csv` is the obvious choice - it's trivial to implement basic CSV parsers in C++ (and
not much harder in FORTRAN or pure C), and most linear algebra libraries (e.g. Eigen, armadillo)
have built in functions for saving to csv.

This comes at a price - an ASCII representations of a double is not generally an exact mapping (though in practice, any error introduced here will be swamped by the error from more mundane things like roundoff) and is woefully space-inefficient: All of the precision available from a double renders into something like `-1.0123456789012345e+1001`, which is **24 bytes** to store 8 bytes of data.
If you need to 


Another major downside is that your data has to be massaged into a 2D tabular format, severely restricting the
physical applications this is suitable for.


For Model A, one could structure a file containing the observable $\langle \mathbb{r} _ 1 + \mathbb{r} _ 2
\rangle$
```
# 2022-04-02 14:44 GMT V=parabola.csv n=2 n_states=11
# Energy | \<psi|x1 + x2|psi\>
 -11.11229 | 0.298329 ...
```

### Raw binary files

The C standard library function `fwrite` lets you just dump the ram onto the disk and 






<td>  </td>
<tdLossy, save/load is expensive, buckles when
data is 3D or larger|
| Raw binary |  Easy to Implement, Lossless, fast save/load, somewhat human readable with `od-F` | Endianness issues, documentation must be in other
files, reading programs need to know data structure (not self contained) |
| .npy  | Efficient, Portable, Readable by numpy | Requires a library in C/C++/FORTRAN, does not
include documentation | 
| hdf5 | Efficient, portable, standardised |








## Parallel Monte Carlo


